name: Gladia Builder

on:
  pull_request:
    branches: [main]

concurrency:
  group: "${{ github.workflow }} @ ${{ github.head_ref || github.ref }}"
  cancel-in-progress: true

env:
  DOCKER_BUILD_NO_CACHE: 'false'
  DOCKER_BUILD_IMAGE_PATH: gladia/gladia
  DOCKER_BUILD_IMAGE_TAG: ci-${{ github.event.pull_request.number }}
  DOCKER_RETENTION_HOURS: 72
  TEST_DEFAULT_MODELS_ONLY: "--default-models-only"
  TEST_DEFAULT_INPUTS_ONLY: "--default-inputs-only"
  TEST_STOP_AT_FAIL: "-x"
  TEST_MARKERS: "-m mandatory"

jobs:
  # if a pull request check the title using a pr linter
  lint:
    if: github.event_name == 'pull_request'
    uses: gladiaio/gladia/.github/workflows/pr-linter.yml@main

  # build the gladia image for the pull request breanch
  build:
    needs: [lint]
    if: |
      !contains(github.event.pull_request.labels.*.name, 'type: ci') &&
      !contains(github.event.pull_request.labels.*.name, 'type: documentation')
    runs-on: [self-hosted, build, persist]
    steps:
      - name: Checkout
        id: checkout
        uses: actions/checkout@v2

      - name: Force docker cache invalidation
        if: |
          contains(github.event.pull_request.labels.*.name, 'ci: force-no-cache')
        run: |
          echo "DOCKER_BUILD_NO_CACHE='true'" >> $GITHUB_ENV

      - name: assert ENV
        run: |
          echo "DOCKER_GLADIA_FQDN=$DOCKER_GLADIA_FQDN" >> $GITHUB_ENV

      - name: Build and push
        uses: docker/build-push-action@v3
        with:
          context: ./src
          file: ./src/docker/lite.Dockerfile
          push: true
          no-cache: ${{env.DOCKER_BUILD_NO_CACHE}}
          build-args: |
            GLADIA_BUILD=${{ github.event.pull_request.number }}
          tags: ${{env.DOCKER_GLADIA_FQDN}}/${{env.DOCKER_BUILD_IMAGE_PATH}}:${{env.DOCKER_BUILD_IMAGE_TAG}}

      - name: Docker cleanup
        run: docker system prune -f -a --filter "until=${{env.DOCKER_RETENTION_HOURS}}h"

  test:
    needs: build
    runs-on: [self-hosted, gladia, test]
    timeout-minutes: 90
    steps:
      # Prepare and start test env
      - name: Assert Template and NS
        run: |
          sed "s/PRN/${{env.DOCKER_BUILD_IMAGE_TAG}}/g" /ci/test-aipi-podTemplate.yml > /tmp/test.yml

      - name: Start test pod
        run: |
          curl -L -o /tmp/kubectl https://storage.googleapis.com/kubernetes-release/release/${KVERSION}/bin/linux/amd64/kubectl
          chmod +x /tmp/kubectl
          /tmp/kubectl apply -f /tmp/test.yml -n $KNS

      # Set test parameters
      - name: Set env variables to select which tests to execute
        if: |
          contains(github.event.pull_request.labels.*.name, 'ci: force-execute-all-test')
        run: |
          echo "TEST_MARKERS=" >> $GITHUB_ENV

      - name: Set env variables to select inputs to test
        if: |
          contains(github.event.pull_request.labels.*.name, 'ci: force-test-all-inputs')
        run: |
          echo "TEST_DEFAULT_INPUTS_ONLY=" >> $GITHUB_ENV

      - name: Set env variables for tests break conditions
        if: |
          contains(github.event.pull_request.labels.*.name, 'ci: test-continue-when-fail')
        run: |
          echo "TEST_STOP_AT_FAIL=" >> $GITHUB_ENV

      - name: Set env variables to select which models to test
        if: |
          contains(github.event.pull_request.labels.*.name, 'ci: force-test-all-models')
        run: |
          echo "TEST_DEFAULT_MODELS_ONLY=" >> $GITHUB_ENV

      # Wait for pod to start
      - name: Gladia container readiness
        run: |
          tryInit=200
          while [[ $(/tmp/kubectl -n gladia-test-aipi get pod aipi-test-${{env.DOCKER_BUILD_TAG}} -o=jsonpath='{.status.conditions[?(@.type=="PodScheduled")].status}') != "False" ]]; do
              if [ $tryInit -eq 0 ]; then
                printf "\nError: TIme out trying to scheduled the test pod. We might be out of GPUs. Retry and/or contact your favorite DevOps <3."
                exit 1
              fi
              if [[ $(/tmp/kubectl -n gladia-test-aipi get pod aipi-test-${{env.DOCKER_BUILD_TAG}}) =~ "NotFound" ]]
                printf "\nError: Test pod disappeared while being scheduled. It probably failed to start. Check logs and/or test your build manually."
                exit 1
              fi
              printf "."
              ((tryInit--))
              sleep 10
          done

          tryReady=200
          while [[ $(/tmp/kubectl -n gladia-test-aipi get pod aipi-test-${{env.DOCKER_BUILD_TAG}} -o=jsonpath='{.status.conditions[?(@.type=="Ready")].status}') != "False" ]]; do
              if [ $tryReady -eq 0 ]; then
                printf "\nError: Test pod timed out while waiting for readyness. Check logs and/or test your build manually."
                exit 1
              fi
              if [[ $(/tmp/kubectl -n gladia-test-aipi get pod aipi-test-${{env.DOCKER_BUILD_TAG}}) =~ "NotFound" ]]
                printf "\nError: Test pod disappeared while starting. Check logs and/or test your build manually."
                exit 1
              fi
              printf "."
              ((tryReady--))
              sleep 10
          done

          # Todo : test url

      - name: Run autogenerated tests
        run: >-
          /tmp/kubectl exec -i -n $KNS aipi-test-${{env.DOCKER_BUILD_TAG}} --
          /bin/bash -c 'eval "$(micromamba shell hook --shell=bash)" && micromamba activate server && python -m gladia_api_utils.tester ${{ env.TEST_DEFAULT_MODELS_ONLY }} ${{ env.TEST_DEFAULT_INPUTS_ONLY }} ${{ env.TEST_STOP_AT_FAIL }} ${{ env.TEST_MARKERS }}'

      - name: Run custom tests
        run: >-
          /tmp/kubectl exec -i -n $KNS aipi-test-${{env.DOCKER_BUILD_TAG}} --
          /bin/bash -c 'eval "$(micromamba shell hook --shell=bash)" && micromamba activate server && python -m pytest -o log_cli=true --log-cli-level=DEBUG ./apis ${{ env.TEST_DEFAULT_MODELS_ONLY }} ${{ env.TEST_DEFAULT_INPUTS_ONLY }} ${{ env.TEST_STOP_AT_FAIL }} ${{ env.TEST_MARKERS }}'

      - name: Get Logs
        if: always()
        run: |
          /tmp/kubectl logs aipi-test-${{env.DOCKER_BUILD_TAG}} -n $KNS

      # we need to docker run to remove root artefact directories
      # this should be done better in the future.
      - name: Clean test pod
        if: always()
        continue-on-error: True
        run: |
          /tmp/kubectl delete -f /tmp/test.yml -n $KNS
